{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_translated_token_emb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "duHGGbwVAhhL"
      },
      "source": [
        "!pip install nlp transformers texttable &> /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AArjDixQAvZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4581c031-b7ed-497b-db7a-71045cb7edbe"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "from typing import List\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)  \n",
        "from texttable import Texttable\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\n",
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel, BertConfig, \\\n",
        "     AdamW, set_seed, AutoConfig, PreTrainedTokenizer, DataCollator, PreTrainedModel, PreTrainedTokenizer, DataCollator, PreTrainedModel\n",
        "\n",
        "set_seed(23)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3IyQ_MtPKlP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc2904e-34c4-46a1-9aa1-0a8e5379b1b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgQgo4I4wQZQ"
      },
      "source": [
        "\n",
        " \n",
        "## Processing Data, Defining Data Classes and Collator and other miscellaneous stuff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7qFiKkjBtZ3"
      },
      "source": [
        "# Class to store data samples, text_a, text_b, score\n",
        "@dataclass\n",
        "class Example:\n",
        "    text_a: str\n",
        "    text_b: str\n",
        "    cui_embedding_a: list\n",
        "    cui_embedding_b: list\n",
        "    score: float\n",
        "\n",
        "\n",
        "# lowercase and add space around words, remove unnecessary spaces\n",
        "def pre_process(sentence, cased=False):\n",
        "    sentence = sentence.replace(\":\", \" : \").replace(\"/\", \" / \").replace(\"[\", \" [ \").replace(\"]\", \" ] \").replace(\"(\", \" ( \").replace(\")\", \" ) \").replace(\"\\\"\", \" \\\" \").replace(\"-\", \" - \").replace(\"?\", \" \").lstrip().rstrip()\n",
        "    if cased:\n",
        "      return re.sub(' +',' ', sentence)\n",
        "    return re.sub(' +',' ', sentence).lower()\n",
        "\n",
        "# def extract_cui_embedding(cui_string):\n",
        "#     return list(map(float, cui_string.split(\",\")))\n",
        "\n",
        "\n",
        "# returns test and train arrays as Example Objects\n",
        "# test train split is stratified and 80-20 split\n",
        "def get_data(cased=False):\n",
        "    train_data = \"/content/drive/My Drive/clinical-sts/data/translated_with_cui_embeddings/translated_train_cui_embeddings_ALL.tsv\"\n",
        "    df = pd.read_csv(train_data, sep=\"\\t\", names=[\"sentence_1\", \"sentence_2\", \"similarity_score\", \"label\", \"cuis_1\", \"cuis_2\", \"cui_embedding_1\", \"cui_embedding_2\"], encoding=\"utf-8\")\n",
        "    df[\"sentence_1\"] = df[\"sentence_1\"].apply(lambda sentence: pre_process(sentence, cased))\n",
        "    df[\"sentence_2\"] = df[\"sentence_2\"].apply(lambda sentence: pre_process(sentence, cased))\n",
        "    df[\"input_sample\"] = df[\"sentence_1\"] + \"<SEP>\" + df[\"sentence_2\"] + \"<SEP>\" + df[\"cui_embedding_1\"] + \"<SEP>\" + df[\"cui_embedding_2\"]\n",
        "\n",
        "    # print(df[\"input_sample\"])\n",
        "    # print(\"test\", df[\"cui_embedding_1\"])\n",
        "    # df[\"cui_embedding_1\"] = df[\"cui_embedding_1\"].apply(lambda embedding_str: extract_cui_embedding(embedding_str)) \n",
        "    # df[\"cui_embedding_2\"] = df[\"cui_embedding_2\"].apply(lambda embedding_str: extract_cui_embedding(embedding_str)) \n",
        "\n",
        "\n",
        "\n",
        "    ## stratified binned sampling\n",
        "    min_val = np.amin(df[\"similarity_score\"])\n",
        "    max_val = np.amax(df[\"similarity_score\"])\n",
        "    bins     = np.linspace(start=min_val, stop=max_val, num=10)\n",
        "    y_binned = np.digitize(df[\"similarity_score\"], bins, right=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[\"input_sample\"],\n",
        "        df[\"similarity_score\"], \n",
        "        stratify=y_binned,\n",
        "        test_size=0.2,\n",
        "        random_state=23\n",
        "    )\n",
        "\n",
        "\n",
        "    for sample, similarity_score in zip(X_train, y_train):\n",
        "      try:\n",
        "        # print(\"heyyy\", sample.split(\"<SEP>\")[2].split(\",\"))\n",
        "        xx = list(map(float, sample.split(\"<SEP>\")[2].split(\",\")))\n",
        "        # print(\"yooo\", xx)\n",
        "      except:\n",
        "        print(\"ohnomy\", sample)\n",
        "\n",
        "    train_a_b = [Example(text_a=sample.split(\"<SEP>\")[0], \n",
        "                    text_b=sample.split(\"<SEP>\")[1],\n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_train, y_train)]\n",
        "    train_b_a = [Example(text_a=sample.split(\"<SEP>\")[1], \n",
        "                    text_b=sample.split(\"<SEP>\")[0], \n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_train, y_train)]\n",
        "    train = train_a_b + train_b_a\n",
        "\n",
        "    test_a_b = [Example(text_a=sample.split(\"<SEP>\")[0], \n",
        "                    text_b=sample.split(\"<SEP>\")[1], \n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_test, y_test)]\n",
        "    test_b_a = [Example(text_a=sample.split(\"<SEP>\")[1], \n",
        "                    text_b=sample.split(\"<SEP>\")[0], \n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_test, y_test)]\n",
        "    test = test_a_b + test_b_a\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DYNAMIC PADDING AND UNIFORM LENGTH BATCHING - reduces wasted computation and makes it faster to run\n",
        "# CODE BORROWED FROM https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n",
        "\n",
        "\n",
        "# We'll be creating a custome dataset using this first\n",
        "@dataclass\n",
        "class Features:\n",
        "    og_sample: Example\n",
        "    cui_embedding: List[float]\n",
        "    input_ids: List[int]\n",
        "    attention_mask: List[int]\n",
        "    score: float\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, \n",
        "                 pad_to_max_length, \n",
        "                 max_len,\n",
        "                 examples: List[Example]):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.examples: List[Example] = examples\n",
        "        self.current = 0\n",
        "        self.pad_to_max_length = pad_to_max_length\n",
        "\n",
        "    # tokenize the sentences and return a Features object for each sentence \n",
        "    def encode(self, ex: Example) -> Features:\n",
        "        encode_dict = self.tokenizer.encode_plus(text=ex.text_a,\n",
        "                                                 text_pair=ex.text_b,\n",
        "                                                 add_special_tokens=True,\n",
        "                                                 max_length=self.max_len,\n",
        "                                                 pad_to_max_length=self.pad_to_max_length,\n",
        "                                                 return_token_type_ids=False,\n",
        "                                                 return_attention_mask=True,\n",
        "                                                 return_overflowing_tokens=False,\n",
        "                                                 return_special_tokens_mask=False,\n",
        "                                                 truncation=True,\n",
        "                                                 )\n",
        "        return Features(og_sample=ex,\n",
        "                        cui_embedding=ex.cui_embedding_a+ex.cui_embedding_b,\n",
        "                        input_ids=encode_dict[\"input_ids\"],\n",
        "                        attention_mask=encode_dict[\"attention_mask\"],\n",
        "                        score=ex.score)\n",
        "\n",
        "    def __getitem__(self, idx) -> Features:\n",
        "        return self.encode(ex=self.examples[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "\n",
        "def pad_seq(seq: List[int], max_batch_len: int, pad_value: int) -> List[int]:\n",
        "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
        "\n",
        "\n",
        "# Smart Collator is used to create batches which are dynamically padded with uniform length \n",
        "@dataclass\n",
        "class SmartCollator:  # (DataCollator):\n",
        "    pad_token_id: int\n",
        "\n",
        "    def collate_batch(self, batch: List[Features]) -> Dict[str, torch.Tensor]:\n",
        "        batch_og_sample = list()\n",
        "        batch_cui_embedding = list()\n",
        "        batch_inputs = list()\n",
        "        batch_attention_masks = list()\n",
        "        labels = list()\n",
        "        max_size = max([len(ex.input_ids) for ex in batch])\n",
        "        for item in batch:\n",
        "            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]\n",
        "            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]\n",
        "            labels.append(item.score)\n",
        "            batch_og_sample.append(item)\n",
        "            batch_cui_embedding.append(np.array(item.cui_embedding))\n",
        "\n",
        "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
        "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
        "                \"score\": torch.tensor(labels, dtype=torch.float),\n",
        "                \"og_sample\": batch_og_sample,\n",
        "                \"cui_embedding\": torch.tensor(batch_cui_embedding, dtype=torch.float)\n",
        "                }\n",
        "                \n",
        "def collate_wrapper(data):\n",
        "    collator = SmartCollator(pad_token_id=tokenizer.pad_token_id)\n",
        "    return collator.collate_batch(data)\n",
        "\n",
        "\n",
        "# USE THIS FUNCTION TO LOAD TEST AND TRAIN DATA AND ITERATE THROUGH THEM\n",
        "def load_data(tokenizer, batch_size, cased=False):\n",
        "    # Get train and test Data Examples\n",
        "    train, test = get_data(cased)\n",
        "\n",
        "\n",
        "    # Now tokenize the words and convert them to token IDs\n",
        "    max_sequence_len = 256\n",
        "    train_set = TextDataset(tokenizer=tokenizer,\n",
        "                            max_len=max_sequence_len,\n",
        "                            examples=train,\n",
        "                            pad_to_max_length=False)\n",
        "\n",
        "    test_set = TextDataset(tokenizer=tokenizer,\n",
        "                            max_len=max_sequence_len,\n",
        "                            examples=test,\n",
        "                            pad_to_max_length=False)\n",
        "\n",
        "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_wrapper,\n",
        "              pin_memory=False, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "\n",
        "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_wrapper,\n",
        "              pin_memory=False, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "    \n",
        "    return train_dataloader, test_dataloader"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJMdJI0Ewkkc"
      },
      "source": [
        "##**Define the Training Loop for fine-tuning the Model.**  \n",
        "We also have some miscellaneous functions to evaluate our model on the dev-set.  \n",
        "\n",
        "  \n",
        "    \n",
        "The model is as shown below. We have different learning rates for the bert and LR layer. Note that we take the hidden layer output from BERT and not the CLS embedding. The CLS embedding does not generate any meaningful sentence embedding and BERT was specifically trained for the NSP task. As such, using the CLS embedding directly leads to worse results. We found that using the penultimate hidden layers gave us best results for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ALszvbWE5Zc"
      },
      "source": [
        "# Let's define the training loop and model\n",
        "\n",
        "def get_bert_output(my_bert, input_ids, attention_mask):    \n",
        "    outputs = my_bert(input_ids, attention_mask=attention_mask)\n",
        "    hidden_states = outputs[2]\n",
        "    sent_embedding = hidden_states[11][:, 0:1, :].squeeze(1).cuda()\n",
        "    return sent_embedding\n",
        "\n",
        "def get_bert_token_mean_output(my_bert, input_ids, attention_mask):  \n",
        "    outputs = my_bert(input_ids, attention_mask=attention_mask)\n",
        "    token_embeddings = outputs[0][:, 1:, :]\n",
        "    attention_mask = attention_mask[:, 1:]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "# class linearRegression(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(linearRegression, self).__init__()\n",
        "#         self.linear = nn.Linear(868, 1)\n",
        "#     def forward(self, x):\n",
        "#         out = self.linear(x)\n",
        "#         return out\n",
        "\n",
        "\n",
        "class linearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear_1 = nn.Sequential(\n",
        "                            nn.Linear(868, 256),\n",
        "                            nn.ReLU())\n",
        "        self.linear_2 = nn.Linear(256, 1)\n",
        "    def forward(self, x):\n",
        "        out = self.linear_1(x)\n",
        "        out = self.linear_2(out)\n",
        "        return out\n",
        "    \n",
        "\n",
        "def run_new_method(my_bert, optimizer, regression_head, regression_optimizer, train_dataloader, test_dataloader, epochs=10, freeze_layers=False):\n",
        "    old_test_loss = float('inf')\n",
        "\n",
        "    # freeze_layers = \"0,1,2,3,4\"\n",
        "    if freeze_layers:\n",
        "        freeze_layers = \"0,1,2,3,4,5\"\n",
        "        layer_indexes = [int(x) for x in freeze_layers.split(\",\")]\n",
        "        for layer_idx in layer_indexes:\n",
        "            for param in list(my_bert.encoder.layer[layer_idx].parameters()):\n",
        "                param.requires_grad = False\n",
        "            print (\"Froze Layer: \", layer_idx)\n",
        "\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "        # put model in train mode\n",
        "        my_bert.train()\n",
        "        regression_head.train()\n",
        "        for step_num, batch_data in enumerate(train_dataloader):\n",
        "            input_ids = batch_data[\"input_ids\"].to(device)\n",
        "            attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "            score = batch_data[\"score\"].to(device)\n",
        "            score = score.unsqueeze(1)\n",
        "            cui_embedding = batch_data[\"cui_embedding\"].to(device)\n",
        "\n",
        "            my_bert_optimizer.zero_grad()\n",
        "            regression_optimizer.zero_grad()\n",
        "            bert_embedding = get_bert_token_mean_output(my_bert, input_ids, attention_mask)\n",
        "\n",
        "            bert_cui_embedding = torch.cat((bert_embedding, cui_embedding), dim=1)\n",
        "            # print(bert_cui_embedding)\n",
        "            predicted_score = regression_head(bert_cui_embedding)\n",
        "\n",
        "            loss_func = nn.MSELoss()\n",
        "            batch_loss = loss_func(predicted_score, score)\n",
        "\n",
        "            \n",
        "            batch_loss.backward()\n",
        "            my_bert_optimizer.step()\n",
        "            regression_optimizer.step()\n",
        "            total_loss += batch_loss.item()\n",
        "            batch_count += 1\n",
        "            train_loss = total_loss/batch_count\n",
        "        print(\"Epoch: {} Train Loss:{}\".format(epoch_num, train_loss))\n",
        "\n",
        "        # put model in test mode\n",
        "        my_bert.eval()\n",
        "        regression_head.eval()\n",
        "        test_loss = 0\n",
        "        test_batch_count = 0\n",
        "        with torch.no_grad():\n",
        "            for step_num, batch_data in enumerate(test_dataloader):\n",
        "                input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                score = batch_data[\"score\"].to(device)\n",
        "                score = score.unsqueeze(1)\n",
        "                cui_embedding = batch_data[\"cui_embedding\"].to(device)\n",
        "\n",
        "                bert_embedding = get_bert_token_mean_output(my_bert, input_ids, attention_mask)\n",
        "                bert_cui_embedding = torch.cat((bert_embedding, cui_embedding), dim=1)\n",
        "                # print(bert_cui_embedding)\n",
        "                predicted_score = regression_head(bert_cui_embedding)\n",
        "                #predicted_score = regression_head(bert_embedding)\n",
        "\n",
        "                loss_func = nn.MSELoss()\n",
        "                batch_loss = loss_func(predicted_score, score)\n",
        "                test_loss += batch_loss.item()\n",
        "                test_batch_count += 1\n",
        "        curr_test_loss = test_loss/test_batch_count\n",
        "        print(\"Epoch: {} Test Loss:{}\\n\".format(epoch_num, curr_test_loss))\n",
        "        # if curr_test_loss < 0.50 or train_loss < 0.25:\n",
        "        #     print(\"yay, exit\")\n",
        "        #     break\n",
        "        # curr_test_loss = test_loss/test_batch_count\n",
        "        # if curr_test_loss-old_test_loss >= 0.03 or curr_test_loss<0.61:\n",
        "        #     print(\"new test loss is greater; breaking\")\n",
        "        #     break\n",
        "        # old_test_loss = curr_test_loss\n",
        "    return my_bert, regression_head\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Method to evaluate and calculate pcc on dev/test dataset, and show terrible predictions\n",
        "'''\n",
        "def evaluate_model(model, regression_head, test_dataloader, show_bad_predictions=True, prediction_difference=2.0):\n",
        "\n",
        "    actual = list()\n",
        "    predicted = list()\n",
        "    og_data = list()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch_count = 0\n",
        "        test_loss =0\n",
        "        for step_num, batch_data in enumerate(test_dataloader):\n",
        "            input_ids = batch_data[\"input_ids\"].to(device)\n",
        "            attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "            score = batch_data[\"score\"].to(device)\n",
        "            score = score.unsqueeze(1)\n",
        "\n",
        "            cui_embedding = batch_data[\"cui_embedding\"].to(device)\n",
        "            bert_embedding = get_bert_token_mean_output(model, input_ids, attention_mask)\n",
        "            bert_cui_embedding = torch.cat((bert_embedding, cui_embedding), dim=1)\n",
        "            predicted_score = regression_head(bert_cui_embedding)\n",
        "\n",
        "            actual.extend(score.tolist())\n",
        "            predicted.extend(predicted_score.tolist())\n",
        "            original_samples = batch_data[\"og_sample\"]\n",
        "            og_data.extend(original_samples)\n",
        "\n",
        "    # show bad predictions in a table\n",
        "    table = Texttable()\n",
        "    table.add_row([\"Actual\", \"Predicted\", \"Difference\", \"Text Sample\"])\n",
        "\n",
        "    for act, pre, og_data in zip(actual, predicted, og_data):\n",
        "        if abs(pre[0]-act[0]) > prediction_difference:\n",
        "            og = og_data.og_sample.text_a + \"    |||    \" + og_data.og_sample.text_b\n",
        "            print(\"{:.2f}    {:.2f}          {:.2f}     {}\".format(act[0], pre[0], abs(pre[0]-act[0]), og))\n",
        "\n",
        "    print('\\n\\n\\n')\n",
        "    actual = [item[0] for item in actual]\n",
        "    predicted = [item[0] for item in predicted]\n",
        "\n",
        "    correlation, p_value = pearsonr(actual, predicted)\n",
        "    print(correlation)\n",
        "\n",
        "    d = {\"a\": actual, \"p\": predicted}\n",
        "    dx = pd.DataFrame(d)\n",
        "    dx.plot.hist(bins=20, alpha=0.25)\n",
        "    return correlation\n",
        "\n",
        "\n",
        "def get_optimizer_params(model):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    opt_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "    return opt_parameters\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9u2XL6HB6AZ"
      },
      "source": [
        "## **Select the base model to fine tune, and pass it to the train loop**\n",
        "We then evaluate the PCC on the dev set.  \n",
        "Note: Only bad examples and scores are printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "3rg2wS20CThe",
        "outputId": "93c434ac-53f5-4952-febe-bb52c664540e"
      },
      "source": [
        "MODEL = \"bert-base-uncased\"\n",
        "my_bert = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert.cuda()\n",
        "\n",
        "regression_head = linearRegression().cuda()\n",
        "regression_optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL, output_hidden_states=True)\n",
        "train_dataloader, test_dataloader = load_data(tokenizer=tokenizer, batch_size=8)\n",
        "bert_config = BertConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert_optimizer = AdamW(get_optimizer_params(my_bert), lr=1e-5)\n",
        "updated_model, regression_head = run_new_method(my_bert=my_bert, optimizer=my_bert_optimizer, # optimizer\n",
        "                                                regression_head=regression_head, \n",
        "                                                regression_optimizer=regression_optimizer,\n",
        "                                                train_dataloader=train_dataloader,\n",
        "                                                test_dataloader=test_dataloader,\n",
        "                                                epochs=3)\n",
        "\n",
        "pcc = evaluate_model(updated_model, regression_head, test_dataloader)\n",
        "model_loc = \"/content/drive/My Drive/clinical-sts/models/tr-token-dnn-bert-{}-{:.4f}.pth\".format(MODEL, pcc)\n",
        "regression_loc = \"/content/drive/My Drive/clinical-sts/models/tr-token-dnn-regression-{}-{:.4f}.pth\".format(MODEL, pcc)\n",
        "torch.save(updated_model, model_loc)\n",
        "torch.save(regression_head, regression_loc)\n",
        "print(\"Saved at:\\n'{}'\\n'{}'\".format(model_loc, regression_loc))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train Loss:0.93595891643055\n",
            "Epoch: 0 Test Loss:0.5739795978468164\n",
            "\n",
            "Epoch: 1 Train Loss:0.37982024230584077\n",
            "Epoch: 1 Test Loss:0.4466990473351084\n",
            "\n",
            "yay, exit\n",
            "0.00    2.10          2.10     aleve 220 mg tablet 2 tablets by mouth two times a day as needed .    |||    serevent diskus 50 mcg / dose disk 1 puff by inhalation two times a day .\n",
            "2.00    4.16          2.16     patient's wife verbalized understanding and had no further questions or concerns at this time .    |||    no further questions or concerns at this time .\n",
            "0.00    2.09          2.09     serevent diskus 50 mcg / dose disk 1 puff by inhalation two times a day .    |||    aleve 220 mg tablet 2 tablets by mouth two times a day as needed .\n",
            "2.00    4.17          2.17     no further questions or concerns at this time .    |||    patient's wife verbalized understanding and had no further questions or concerns at this time .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.8892692212428632\n",
            "Saved at:\n",
            "'/content/drive/My Drive/clinical-sts/models/tr-token-dnn-bert-bert-base-uncased-0.8893.pth'\n",
            "'/content/drive/My Drive/clinical-sts/models/tr-token-dnn-regression-bert-base-uncased-0.8893.pth'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "cw3x6GbNvfNB",
        "outputId": "a7215170-e717-4a64-d860-05f39d638650"
      },
      "source": [
        "MODEL = \"/content/drive/My Drive/clinical-sts/bio-bert/\"\n",
        "my_bert = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert.cuda()\n",
        "\n",
        "regression_head = linearRegression().cuda()\n",
        "regression_optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, output_hidden_states=True)\n",
        "train_dataloader, test_dataloader = load_data(tokenizer=tokenizer, batch_size=8)\n",
        "bert_config = BertConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert_optimizer = AdamW(get_optimizer_params(my_bert), lr=1e-5)\n",
        "updated_model, regression_head = run_new_method(my_bert=my_bert, optimizer=my_bert_optimizer, # optimizer\n",
        "                                                regression_head=regression_head, \n",
        "                                                regression_optimizer=regression_optimizer,\n",
        "                                                train_dataloader=train_dataloader,\n",
        "                                                test_dataloader=test_dataloader,\n",
        "                                                epochs=4)\n",
        "\n",
        "pcc = evaluate_model(updated_model, regression_head, test_dataloader)\n",
        "model_name = \"bio-bert\"\n",
        "model_loc = \"/content/drive/My Drive/clinical-sts/models/tr-token-dnn-bert-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "regression_loc = \"/content/drive/My Drive/clinical-sts/models/tr-token-dnn-regression-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "torch.save(updated_model, model_loc)\n",
        "torch.save(regression_head, regression_loc)\n",
        "print(\"Saved at:\\n'{}'\\n'{}'\".format(model_loc, regression_loc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train Loss:0.8662502449322739\n",
            "Epoch: 0 Test Loss:0.49268347747567903\n",
            "\n",
            "Epoch: 1 Train Loss:0.27299450995974484\n",
            "Epoch: 1 Test Loss:0.44070686024606676\n",
            "\n",
            "Epoch: 2 Train Loss:0.14604029006241964\n",
            "Epoch: 2 Test Loss:0.3949959694190805\n",
            "\n",
            "Epoch: 3 Train Loss:0.094148926252045\n",
            "Epoch: 3 Test Loss:0.3654993254467285\n",
            "\n",
            "3.45    1.29          2.16     goals : patient will demonstrate a 50 % decrease in pain in 6 sessions for increased participation in life or work activities .    |||    patient will demonstrate and / or verbalize understanding of home exercise program in 6 sessions for improved self management of the condition .\n",
            "2.50    0.43          2.07     the patient expressed understanding and consent to this plan.    |||    i have reviewed the history, checked the patients, discussed treatments, and agreed to dr. name\n",
            "1.00    3.05          2.05     the gastrointestinal examination system was negative, and the historian denied abdominal pain, nausea, and vomiting.    |||    negative ear, nose, and throat review system, historians deny earache, sore throat, and stridor.\n",
            "3.45    1.37          2.08     patient will demonstrate and / or verbalize understanding of home exercise program in 6 sessions for improved self management of the condition .    |||    goals : patient will demonstrate a 50 % decrease in pain in 6 sessions for increased participation in life or work activities .\n",
            "2.50    0.46          2.04     i have reviewed the history, checked the patients, discussed treatments, and agreed to dr. name    |||    the patient expressed understanding and consent to this plan.\n",
            "1.00    3.38          2.38     negative ear, nose, and throat review system, historians deny earache, sore throat, and stridor.    |||    the gastrointestinal examination system was negative, and the historian denied abdominal pain, nausea, and vomiting.\n",
            "4.00    1.98          2.02     a straight chest is not good for chest pain, pressure or tightness, swelling of legs or feet, and shortness of breath.    |||    respiratory tract : cough, hemoptysis, chest tightness, shortness of breath and wheezing were negative.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.9014246951170913\n",
            "Saved at:\n",
            "'/content/drive/My Drive/clinical-sts/models/tr-token-dnn-bert-bio-bert-0.9014.pth'\n",
            "'/content/drive/My Drive/clinical-sts/models/tr-token-dnn-regression-bio-bert-0.9014.pth'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "Txif48YgW5hG",
        "outputId": "d768db06-96ce-49c6-8eb2-4f1b50dcea68"
      },
      "source": [
        "MODEL = \"/content/drive/My Drive/clinical-sts/umls-bert/\"\n",
        "my_bert = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert.cuda()\n",
        "\n",
        "regression_head = linearRegression().cuda()\n",
        "regression_optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, output_hidden_states=True)\n",
        "train_dataloader, test_dataloader = load_data(tokenizer=tokenizer, batch_size=8)\n",
        "bert_config = BertConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert_optimizer = AdamW(get_optimizer_params(my_bert), lr=1e-5)\n",
        "updated_model, regression_head = run_new_method(my_bert=my_bert, optimizer=my_bert_optimizer, # optimizer\n",
        "                                                regression_head=regression_head, \n",
        "                                                regression_optimizer=regression_optimizer,\n",
        "                                                train_dataloader=train_dataloader,\n",
        "                                                test_dataloader=test_dataloader,\n",
        "                                                epochs=4)\n",
        "\n",
        "pcc = evaluate_model(updated_model, regression_head, test_dataloader)\n",
        "model_name = \"umls-bert\"\n",
        "model_loc = \"/content/drive/My Drive/clinical-sts/models/tr-token-dnn-bert-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "regression_loc = \"/content/drive/My Drive/clinical-sts/models/tr-token-dnn-regression-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "torch.save(updated_model, model_loc)\n",
        "torch.save(regression_head, regression_loc)\n",
        "print(\"Saved at:\\n'{}'\\n'{}'\".format(model_loc, regression_loc))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/clinical-sts/umls-bert/ were not used when initializing BertModel: ['bert.embeddings.tui_type_embeddings.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train Loss:0.9405643974337551\n",
            "Epoch: 0 Test Loss:0.546940212299053\n",
            "\n",
            "Epoch: 1 Train Loss:0.314505549760391\n",
            "Epoch: 1 Test Loss:0.5021767306484675\n",
            "\n",
            "Epoch: 2 Train Loss:0.1835423644516786\n",
            "Epoch: 2 Test Loss:0.4998021124953166\n",
            "\n",
            "Epoch: 3 Train Loss:0.10876326754492037\n",
            "Epoch: 3 Test Loss:0.5265016195021177\n",
            "\n",
            "1.50    3.69          2.19     musculoskeletal : positive for gait problem , joint swelling and extremity pain .    |||    musculoskeletal : negative for back pain , myalgias and extremity pain .\n",
            "0.00    2.61          2.61     the patient related understanding of the proper wearing and care of the splint ( s ) and was instructed to call with any problems .    |||    plan : the patient stated an understanding of the program , and agrees to continue independently with a home management program .\n",
            "0.00    2.05          2.05     patient repeats back dosing instructions and has no further questions at this time .    |||    the patient does not wish to enroll at this time .\n",
            "1.00    3.40          2.40     the gastrointestinal examination system was negative, and the historian denied abdominal pain, nausea, and vomiting.    |||    negative ear, nose, and throat review system, historians deny earache, sore throat, and stridor.\n",
            "0.50    2.53          2.03     the patient is a student, engaged in sports.    |||    general : the patient is alert and oriented 3 times, without obvious discomfort.\n",
            "1.50    3.60          2.10     musculoskeletal : for gait problems, joint swelling and limb pain are positive.    |||    musculoskeletal : negative for back pain, myalgia, and limb pain.\n",
            "0.00    2.23          2.23     the diagnosis and treatment plans were explained and the patient expressed understanding of the content .    |||    i encouraged the patient to actively pursue a level of understanding and comfort with her sexuality .\n",
            "2.90    4.91          2.01     no : itching or clear fluid - filled or pus - filled blisters / sores on the rash that are either still in place or broken    |||    clear fluid - filled or pus - filled blisters / sores on the rash that are either still in place or broken\n",
            "0.50    3.17          2.67     the patient is a student and involved in sports .    |||    general : the patient is alert and oriented x 3 and in no apparent distress .\n",
            "1.50    3.71          2.21     musculoskeletal : negative for back pain , myalgias and extremity pain .    |||    musculoskeletal : positive for gait problem , joint swelling and extremity pain .\n",
            "0.00    2.28          2.28     plan : the patient stated an understanding of the program , and agrees to continue independently with a home management program .    |||    the patient related understanding of the proper wearing and care of the splint ( s ) and was instructed to call with any problems .\n",
            "2.90    4.99          2.09     clear blisters / sores filled with fluid or pus on the rash, which are still in place or ruptured    |||    no : itchy or transparent fluid - filled or pus - filled blisters / abscesses on the rash are still present or ruptured\n",
            "1.00    3.72          2.72     negative ear, nose, and throat review system, historians deny earache, sore throat, and stridor.    |||    the gastrointestinal examination system was negative, and the historian denied abdominal pain, nausea, and vomiting.\n",
            "1.00    3.09          2.09     calcium 600 with vitamin d3 tablet 1 tablet by mouth one time daily .    |||    pantoprazole [ protonix ] 40 mg tablet enteric coated 1 tablet by mouth one time daily .\n",
            "1.50    3.75          2.25     musculoskeletal : negative for back pain, myalgia, and limb pain.    |||    musculoskeletal : for gait problems, joint swelling and limb pain are positive.\n",
            "0.00    2.14          2.14     i encouraged the patient to actively pursue a level of understanding and comfort with her sexuality .    |||    the diagnosis and treatment plans were explained and the patient expressed understanding of the content .\n",
            "2.90    5.05          2.15     clear fluid - filled or pus - filled blisters / sores on the rash that are either still in place or broken    |||    no : itching or clear fluid - filled or pus - filled blisters / sores on the rash that are either still in place or broken\n",
            "0.50    3.11          2.61     general : the patient is alert and oriented x 3 and in no apparent distress .    |||    the patient is a student and involved in sports .\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.8928605379325314\n",
            "Saved at:\n",
            "'/content/drive/My Drive/clinical-sts/models/tr-token-dnn-bert-umls-bert-0.8929.pth'\n",
            "'/content/drive/My Drive/clinical-sts/models/tr-token-dnn-regression-umls-bert-0.8929.pth'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_w3lqbvasEz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}