{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clinical_sts_train_part2-token-embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "duHGGbwVAhhL"
      },
      "source": [
        "!pip install nlp transformers texttable &> /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AArjDixQAvZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5f2be2-b018-45f4-cecf-e7b912298f81"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "from typing import List\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)  \n",
        "from texttable import Texttable\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\n",
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel, BertConfig, \\\n",
        "     AdamW, set_seed, AutoConfig, PreTrainedTokenizer, DataCollator, PreTrainedModel, PreTrainedTokenizer, DataCollator, PreTrainedModel\n",
        "\n",
        "set_seed(23)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3IyQ_MtPKlP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5032230-bad3-4b2e-9c69-0558146bf02a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgQgo4I4wQZQ"
      },
      "source": [
        "\n",
        " \n",
        "## Processing Data, Defining Data Classes and Collator and other miscellaneous stuff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7qFiKkjBtZ3"
      },
      "source": [
        "# Class to store data samples, text_a, text_b, score\n",
        "@dataclass\n",
        "class Example:\n",
        "    text_a: str\n",
        "    text_b: str\n",
        "    cui_embedding_a: list\n",
        "    cui_embedding_b: list\n",
        "    score: float\n",
        "\n",
        "\n",
        "# lowercase and add space around words, remove unnecessary spaces\n",
        "def pre_process(sentence, cased=False):\n",
        "    sentence = sentence.replace(\":\", \" : \").replace(\"/\", \" / \").replace(\"[\", \" [ \").replace(\"]\", \" ] \").replace(\"(\", \" ( \").replace(\")\", \" ) \").replace(\"\\\"\", \" \\\" \").replace(\"-\", \" - \").replace(\"?\", \" \").lstrip().rstrip()\n",
        "    if cased:\n",
        "      return re.sub(' +',' ', sentence)\n",
        "    return re.sub(' +',' ', sentence).lower()\n",
        "\n",
        "# def extract_cui_embedding(cui_string):\n",
        "#     return list(map(float, cui_string.split(\",\")))\n",
        "\n",
        "\n",
        "# returns test and train arrays as Example Objects\n",
        "# test train split is stratified and 80-20 split\n",
        "def get_data(cased=False):\n",
        "    train_data = \"/content/drive/My Drive/clinical-sts/augmented_train_with_cui_embeddings.tsv\"\n",
        "    df = pd.read_csv(train_data, sep=\"\\t\", names=[\"sentence_1\", \"sentence_2\", \"similarity_score\", \"label\", \"cuis_1\", \"cuis_2\", \"cui_embedding_1\", \"cui_embedding_2\"], encoding=\"utf-8\")\n",
        "    df[\"sentence_1\"] = df[\"sentence_1\"].apply(lambda sentence: pre_process(sentence, cased))\n",
        "    df[\"sentence_2\"] = df[\"sentence_2\"].apply(lambda sentence: pre_process(sentence, cased))\n",
        "    df[\"input_sample\"] = df[\"sentence_1\"] + \"<SEP>\" + df[\"sentence_2\"] + \"<SEP>\" + df[\"cui_embedding_1\"] + \"<SEP>\" + df[\"cui_embedding_2\"]\n",
        "\n",
        "    # print(df[\"input_sample\"])\n",
        "    # print(\"test\", df[\"cui_embedding_1\"])\n",
        "    # df[\"cui_embedding_1\"] = df[\"cui_embedding_1\"].apply(lambda embedding_str: extract_cui_embedding(embedding_str)) \n",
        "    # df[\"cui_embedding_2\"] = df[\"cui_embedding_2\"].apply(lambda embedding_str: extract_cui_embedding(embedding_str)) \n",
        "\n",
        "\n",
        "\n",
        "    ## stratified binned sampling\n",
        "    min_val = np.amin(df[\"similarity_score\"])\n",
        "    max_val = np.amax(df[\"similarity_score\"])\n",
        "    bins     = np.linspace(start=min_val, stop=max_val, num=10)\n",
        "    y_binned = np.digitize(df[\"similarity_score\"], bins, right=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df[\"input_sample\"],\n",
        "        df[\"similarity_score\"], \n",
        "        stratify=y_binned,\n",
        "        test_size=0.2,\n",
        "        random_state=23\n",
        "    )\n",
        "\n",
        "\n",
        "    for sample, similarity_score in zip(X_train, y_train):\n",
        "      try:\n",
        "        # print(\"heyyy\", sample.split(\"<SEP>\")[2].split(\",\"))\n",
        "        xx = list(map(float, sample.split(\"<SEP>\")[2].split(\",\")))\n",
        "        # print(\"yooo\", xx)\n",
        "      except:\n",
        "        print(\"ohnomy\", sample)\n",
        "\n",
        "    train_a_b = [Example(text_a=sample.split(\"<SEP>\")[0], \n",
        "                    text_b=sample.split(\"<SEP>\")[1],\n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_train, y_train)]\n",
        "    train_b_a = [Example(text_a=sample.split(\"<SEP>\")[1], \n",
        "                    text_b=sample.split(\"<SEP>\")[0], \n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_train, y_train)]\n",
        "    train = train_a_b + train_b_a\n",
        "\n",
        "    test_a_b = [Example(text_a=sample.split(\"<SEP>\")[0], \n",
        "                    text_b=sample.split(\"<SEP>\")[1], \n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_test, y_test)]\n",
        "    test_b_a = [Example(text_a=sample.split(\"<SEP>\")[1], \n",
        "                    text_b=sample.split(\"<SEP>\")[0], \n",
        "                    cui_embedding_a = list(map(float, sample.split(\"<SEP>\")[3].split(\",\"))),\n",
        "                    cui_embedding_b = list(map(float, sample.split(\"<SEP>\")[2].split(\",\"))),\n",
        "                    score=similarity_score) for sample, similarity_score in zip(X_test, y_test)]\n",
        "    test = test_a_b + test_b_a\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DYNAMIC PADDING AND UNIFORM LENGTH BATCHING - reduces wasted computation and makes it faster to run\n",
        "# CODE BORROWED FROM https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n",
        "\n",
        "\n",
        "# We'll be creating a custome dataset using this first\n",
        "@dataclass\n",
        "class Features:\n",
        "    og_sample: Example\n",
        "    cui_embedding: List[float]\n",
        "    input_ids: List[int]\n",
        "    attention_mask: List[int]\n",
        "    score: float\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, \n",
        "                 pad_to_max_length, \n",
        "                 max_len,\n",
        "                 examples: List[Example]):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.examples: List[Example] = examples\n",
        "        self.current = 0\n",
        "        self.pad_to_max_length = pad_to_max_length\n",
        "\n",
        "    # tokenize the sentences and return a Features object for each sentence \n",
        "    def encode(self, ex: Example) -> Features:\n",
        "        encode_dict = self.tokenizer.encode_plus(text=ex.text_a,\n",
        "                                                 text_pair=ex.text_b,\n",
        "                                                 add_special_tokens=True,\n",
        "                                                 max_length=self.max_len,\n",
        "                                                 pad_to_max_length=self.pad_to_max_length,\n",
        "                                                 return_token_type_ids=False,\n",
        "                                                 return_attention_mask=True,\n",
        "                                                 return_overflowing_tokens=False,\n",
        "                                                 return_special_tokens_mask=False,\n",
        "                                                 truncation=True,\n",
        "                                                 )\n",
        "        return Features(og_sample=ex,\n",
        "                        cui_embedding=ex.cui_embedding_a+ex.cui_embedding_b,\n",
        "                        input_ids=encode_dict[\"input_ids\"],\n",
        "                        attention_mask=encode_dict[\"attention_mask\"],\n",
        "                        score=ex.score)\n",
        "\n",
        "    def __getitem__(self, idx) -> Features:\n",
        "        return self.encode(ex=self.examples[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "\n",
        "def pad_seq(seq: List[int], max_batch_len: int, pad_value: int) -> List[int]:\n",
        "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
        "\n",
        "\n",
        "# Smart Collator is used to create batches which are dynamically padded with uniform length \n",
        "@dataclass\n",
        "class SmartCollator:  # (DataCollator):\n",
        "    pad_token_id: int\n",
        "\n",
        "    def collate_batch(self, batch: List[Features]) -> Dict[str, torch.Tensor]:\n",
        "        batch_og_sample = list()\n",
        "        batch_cui_embedding = list()\n",
        "        batch_inputs = list()\n",
        "        batch_attention_masks = list()\n",
        "        labels = list()\n",
        "        max_size = max([len(ex.input_ids) for ex in batch])\n",
        "        for item in batch:\n",
        "            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]\n",
        "            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]\n",
        "            labels.append(item.score)\n",
        "            batch_og_sample.append(item)\n",
        "            batch_cui_embedding.append(np.array(item.cui_embedding))\n",
        "\n",
        "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
        "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
        "                \"score\": torch.tensor(labels, dtype=torch.float),\n",
        "                \"og_sample\": batch_og_sample,\n",
        "                \"cui_embedding\": torch.tensor(batch_cui_embedding, dtype=torch.float)\n",
        "                }\n",
        "                \n",
        "def collate_wrapper(data):\n",
        "    collator = SmartCollator(pad_token_id=tokenizer.pad_token_id)\n",
        "    return collator.collate_batch(data)\n",
        "\n",
        "\n",
        "# USE THIS FUNCTION TO LOAD TEST AND TRAIN DATA AND ITERATE THROUGH THEM\n",
        "def load_data(tokenizer, batch_size, cased=False):\n",
        "    # Get train and test Data Examples\n",
        "    train, test = get_data(cased)\n",
        "\n",
        "\n",
        "    # Now tokenize the words and convert them to token IDs\n",
        "    max_sequence_len = 256\n",
        "    train_set = TextDataset(tokenizer=tokenizer,\n",
        "                            max_len=max_sequence_len,\n",
        "                            examples=train,\n",
        "                            pad_to_max_length=False)\n",
        "\n",
        "    test_set = TextDataset(tokenizer=tokenizer,\n",
        "                            max_len=max_sequence_len,\n",
        "                            examples=test,\n",
        "                            pad_to_max_length=False)\n",
        "\n",
        "    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_wrapper,\n",
        "              pin_memory=False, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "\n",
        "    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_wrapper,\n",
        "              pin_memory=False, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "    \n",
        "    return train_dataloader, test_dataloader"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJMdJI0Ewkkc"
      },
      "source": [
        "##**Define the Training Loop for fine-tuning the Model.**  \n",
        "We also have some miscellaneous functions to evaluate our model on the dev-set.  \n",
        "\n",
        "  \n",
        "    \n",
        "The model is as shown below. We have different learning rates for the bert and LR layer. Note that we take the hidden layer output from BERT and not the CLS embedding. The CLS embedding does not generate any meaningful sentence embedding and BERT was specifically trained for the NSP task. As such, using the CLS embedding directly leads to worse results. We found that using the penultimate hidden layers gave us best results for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ALszvbWE5Zc"
      },
      "source": [
        "# Let's define the training loop and model\n",
        "\n",
        "def get_bert_output(my_bert, input_ids, attention_mask):    \n",
        "    outputs = my_bert(input_ids, attention_mask=attention_mask)\n",
        "    hidden_states = outputs[2]\n",
        "    sent_embedding = hidden_states[11][:, 0:1, :].squeeze(1).cuda()\n",
        "    return sent_embedding\n",
        "\n",
        "def get_bert_token_mean_output(my_bert, input_ids, attention_mask):  \n",
        "    outputs = my_bert(input_ids, attention_mask=attention_mask)\n",
        "    token_embeddings = outputs[0][:, 1:, :]\n",
        "    attention_mask = attention_mask[:, 1:]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "# class linearRegression(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(linearRegression, self).__init__()\n",
        "#         self.linear = nn.Linear(868, 1)\n",
        "#     def forward(self, x):\n",
        "#         out = self.linear(x)\n",
        "#         return out\n",
        "\n",
        "\n",
        "class linearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear_1 = nn.Sequential(\n",
        "                            nn.Linear(868, 256),\n",
        "                            nn.ReLU())\n",
        "        self.linear_2 = nn.Linear(256, 1)\n",
        "    def forward(self, x):\n",
        "        out = self.linear_1(x)\n",
        "        out = self.linear_2(out)\n",
        "        return out\n",
        "    \n",
        "\n",
        "def run_new_method(my_bert, optimizer, regression_head, regression_optimizer, train_dataloader, test_dataloader, epochs=10, freeze_layers=False):\n",
        "    old_test_loss = float('inf')\n",
        "\n",
        "    # freeze_layers = \"0,1,2,3,4\"\n",
        "    if freeze_layers:\n",
        "        freeze_layers = \"0,1,2,3,4,5\"\n",
        "        layer_indexes = [int(x) for x in freeze_layers.split(\",\")]\n",
        "        for layer_idx in layer_indexes:\n",
        "            for param in list(my_bert.encoder.layer[layer_idx].parameters()):\n",
        "                param.requires_grad = False\n",
        "            print (\"Froze Layer: \", layer_idx)\n",
        "\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "        # put model in train mode\n",
        "        my_bert.train()\n",
        "        regression_head.train()\n",
        "        for step_num, batch_data in enumerate(train_dataloader):\n",
        "            input_ids = batch_data[\"input_ids\"].to(device)\n",
        "            attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "            score = batch_data[\"score\"].to(device)\n",
        "            score = score.unsqueeze(1)\n",
        "            cui_embedding = batch_data[\"cui_embedding\"].to(device)\n",
        "\n",
        "            my_bert_optimizer.zero_grad()\n",
        "            regression_optimizer.zero_grad()\n",
        "            bert_embedding = get_bert_token_mean_output(my_bert, input_ids, attention_mask)\n",
        "\n",
        "            bert_cui_embedding = torch.cat((bert_embedding, cui_embedding), dim=1)\n",
        "            # print(bert_cui_embedding)\n",
        "            predicted_score = regression_head(bert_cui_embedding)\n",
        "\n",
        "            loss_func = nn.MSELoss()\n",
        "            batch_loss = loss_func(predicted_score, score)\n",
        "\n",
        "            \n",
        "            batch_loss.backward()\n",
        "            my_bert_optimizer.step()\n",
        "            regression_optimizer.step()\n",
        "            total_loss += batch_loss.item()\n",
        "            batch_count += 1\n",
        "            train_loss = total_loss/batch_count\n",
        "        print(\"Epoch: {} Train Loss:{}\".format(epoch_num, train_loss))\n",
        "\n",
        "        # put model in test mode\n",
        "        my_bert.eval()\n",
        "        regression_head.eval()\n",
        "        test_loss = 0\n",
        "        test_batch_count = 0\n",
        "        with torch.no_grad():\n",
        "            for step_num, batch_data in enumerate(test_dataloader):\n",
        "                input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                score = batch_data[\"score\"].to(device)\n",
        "                score = score.unsqueeze(1)\n",
        "                cui_embedding = batch_data[\"cui_embedding\"].to(device)\n",
        "\n",
        "                bert_embedding = get_bert_token_mean_output(my_bert, input_ids, attention_mask)\n",
        "                bert_cui_embedding = torch.cat((bert_embedding, cui_embedding), dim=1)\n",
        "                # print(bert_cui_embedding)\n",
        "                predicted_score = regression_head(bert_cui_embedding)\n",
        "                #predicted_score = regression_head(bert_embedding)\n",
        "\n",
        "                loss_func = nn.MSELoss()\n",
        "                batch_loss = loss_func(predicted_score, score)\n",
        "                test_loss += batch_loss.item()\n",
        "                test_batch_count += 1\n",
        "        curr_test_loss = test_loss/test_batch_count\n",
        "        print(\"Epoch: {} Test Loss:{}\\n\".format(epoch_num, curr_test_loss))\n",
        "        if curr_test_loss < 0.595 or train_loss < 0.25:\n",
        "            print(\"yay, exit\")\n",
        "            break\n",
        "        # curr_test_loss = test_loss/test_batch_count\n",
        "        # if curr_test_loss-old_test_loss >= 0.03 or curr_test_loss<0.61:\n",
        "        #     print(\"new test loss is greater; breaking\")\n",
        "        #     break\n",
        "        # old_test_loss = curr_test_loss\n",
        "    return my_bert, regression_head\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Method to evaluate and calculate pcc on dev/test dataset, and show terrible predictions\n",
        "'''\n",
        "def evaluate_model(model, regression_head, test_dataloader, show_bad_predictions=True, prediction_difference=2.0):\n",
        "\n",
        "    actual = list()\n",
        "    predicted = list()\n",
        "    og_data = list()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch_count = 0\n",
        "        test_loss =0\n",
        "        for step_num, batch_data in enumerate(test_dataloader):\n",
        "            input_ids = batch_data[\"input_ids\"].to(device)\n",
        "            attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "            score = batch_data[\"score\"].to(device)\n",
        "            score = score.unsqueeze(1)\n",
        "\n",
        "            cui_embedding = batch_data[\"cui_embedding\"].to(device)\n",
        "            bert_embedding = get_bert_token_mean_output(model, input_ids, attention_mask)\n",
        "            bert_cui_embedding = torch.cat((bert_embedding, cui_embedding), dim=1)\n",
        "            predicted_score = regression_head(bert_cui_embedding)\n",
        "\n",
        "            actual.extend(score.tolist())\n",
        "            predicted.extend(predicted_score.tolist())\n",
        "            original_samples = batch_data[\"og_sample\"]\n",
        "            og_data.extend(original_samples)\n",
        "\n",
        "    # show bad predictions in a table\n",
        "    table = Texttable()\n",
        "    table.add_row([\"Actual\", \"Predicted\", \"Difference\", \"Text Sample\"])\n",
        "\n",
        "    for act, pre, og_data in zip(actual, predicted, og_data):\n",
        "        if abs(pre[0]-act[0]) > prediction_difference:\n",
        "            og = og_data.og_sample.text_a + \"    |||    \" + og_data.og_sample.text_b\n",
        "            print(\"{:.2f}    {:.2f}          {:.2f}     {}\".format(act[0], pre[0], abs(pre[0]-act[0]), og))\n",
        "\n",
        "    print('\\n\\n\\n')\n",
        "    actual = [item[0] for item in actual]\n",
        "    predicted = [item[0] for item in predicted]\n",
        "\n",
        "    correlation, p_value = pearsonr(actual, predicted)\n",
        "    print(correlation)\n",
        "\n",
        "    d = {\"a\": actual, \"p\": predicted}\n",
        "    dx = pd.DataFrame(d)\n",
        "    dx.plot.hist(bins=20, alpha=0.25)\n",
        "    return correlation\n",
        "\n",
        "\n",
        "def get_optimizer_params(model):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    opt_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "        'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "    return opt_parameters\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9u2XL6HB6AZ"
      },
      "source": [
        "## **Select the base model to fine tune, and pass it to the train loop**\n",
        "We then evaluate the PCC on the dev set.  \n",
        "Note: Only bad examples and scores are printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFhIRadUrHHX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "eafc882c-9de0-4845-d06a-3e638f70acd6"
      },
      "source": [
        "MODEL = \"bert-base-uncased\"\n",
        "my_bert = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert.cuda()\n",
        "\n",
        "regression_head = linearRegression().cuda()\n",
        "regression_optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL, output_hidden_states=True)\n",
        "train_dataloader, test_dataloader = load_data(tokenizer=tokenizer, batch_size=8)\n",
        "bert_config = BertConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert_optimizer = AdamW(get_optimizer_params(my_bert), lr=1e-5)\n",
        "updated_model, regression_head = run_new_method(my_bert=my_bert, optimizer=my_bert_optimizer, # optimizer\n",
        "                                                regression_head=regression_head, \n",
        "                                                regression_optimizer=regression_optimizer,\n",
        "                                                train_dataloader=train_dataloader,\n",
        "                                                test_dataloader=test_dataloader,\n",
        "                                                epochs=3)\n",
        "\n",
        "pcc = evaluate_model(updated_model, regression_head, test_dataloader)\n",
        "model_loc = \"/content/drive/My Drive/clinical-sts/models/new-token-dnn-bert-{}-{:.4f}.pth\".format(MODEL, pcc)\n",
        "regression_loc = \"/content/drive/My Drive/clinical-sts/models/new-token-dnn-regression-{}-{:.4f}.pth\".format(MODEL, pcc)\n",
        "torch.save(updated_model, model_loc)\n",
        "torch.save(regression_head, regression_loc)\n",
        "print(\"Saved at:\\n'{}'\\n'{}'\".format(model_loc, regression_loc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train Loss:0.9711250443992916\n",
            "Epoch: 0 Test Loss:0.6474846399256161\n",
            "\n",
            "Epoch: 1 Train Loss:0.3802781496196985\n",
            "Epoch: 1 Test Loss:0.5893876313985813\n",
            "\n",
            "Epoch: 2 Train Loss:0.25507130486369933\n",
            "Epoch: 2 Test Loss:0.580667723147642\n",
            "\n",
            "0.00    2.00          2.00     reviewed encounter review of systems and pertinent responses are noted in the history.    |||    the tests and scores are listed at the end of this report.\n",
            "1.00    3.84          2.84     the patient is awake, alert, and oriented times three.    |||    patient is appreciative, understands and is in agreement with plan of care.\n",
            "4.00    1.83          2.17     thank you for contacting the xxx anticoagulation clinic.    |||    thank you for choosing the name, m.d.. care team for your health care needs!\n",
            "1.50    3.50          2.00     , family educational needs - family assessed : ready to learn, no apparent learning barriers.    |||    patient education : ready to learn, no apparent learning barriers were identified; learning preferences include listening.\n",
            "1.00    3.71          2.71     patient is appreciative, understands and is in agreement with plan of care.    |||    the patient is awake, alert, and oriented times three.\n",
            "1.50    3.62          2.12     patient education : ready to learn, no apparent learning barriers were identified; learning preferences include listening.    |||    , family educational needs - family assessed : ready to learn, no apparent learning barriers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.8481655390923607\n",
            "Saved at:\n",
            "'/content/drive/My Drive/clinical-sts/models/new-token-dnn-bert-bert-base-uncased-0.85.pth'\n",
            "'/content/drive/My Drive/clinical-sts/models/new-token-dnn-regression-bert-base-uncased-0.85.pth'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "cw3x6GbNvfNB",
        "outputId": "ba2527a3-9752-4d38-b3f1-a2d372920e13"
      },
      "source": [
        "MODEL = \"/content/drive/My Drive/clinical-sts/bio-bert/\"\n",
        "my_bert = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert.cuda()\n",
        "\n",
        "regression_head = linearRegression().cuda()\n",
        "regression_optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, output_hidden_states=True)\n",
        "train_dataloader, test_dataloader = load_data(tokenizer=tokenizer, batch_size=8)\n",
        "bert_config = BertConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert_optimizer = AdamW(get_optimizer_params(my_bert), lr=1e-5)\n",
        "updated_model, regression_head = run_new_method(my_bert=my_bert, optimizer=my_bert_optimizer, # optimizer\n",
        "                                                regression_head=regression_head, \n",
        "                                                regression_optimizer=regression_optimizer,\n",
        "                                                train_dataloader=train_dataloader,\n",
        "                                                test_dataloader=test_dataloader,\n",
        "                                                epochs=5)\n",
        "\n",
        "pcc = evaluate_model(updated_model, regression_head, test_dataloader)\n",
        "model_name = \"bio-bert\"\n",
        "model_loc = \"/content/drive/My Drive/clinical-sts/models/new-token-dnn-bert-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "regression_loc = \"/content/drive/My Drive/clinical-sts/models/new-token-dnn-regression-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "torch.save(updated_model, model_loc)\n",
        "torch.save(regression_head, regression_loc)\n",
        "print(\"Saved at:\\n'{}'\\n'{}'\".format(model_loc, regression_loc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train Loss:0.9784199135398032\n",
            "Epoch: 0 Test Loss:0.6031265656153361\n",
            "\n",
            "Epoch: 1 Train Loss:0.34212384843618\n",
            "Epoch: 1 Test Loss:0.586924209481194\n",
            "\n",
            "Epoch: 2 Train Loss:0.20561478694059676\n",
            "Epoch: 2 Test Loss:0.6191500109576044\n",
            "\n",
            "yay, exit\n",
            "2.00    4.32          2.32     discussed diagnosis and treatment plan; patient expressed understanding of the content.    |||    they verbalized agreement and understanding of the plan and contact information.\n",
            "1.50    3.65          2.15     instructions : take 2 tablets on first day followed 1 tablet daily on days 2 through 5. take medication for 5 days.    |||    instructions : take 1 tab tid for 3 days, then take 1 tab bid for 3 days, then take 1 tab daily for 3 days, then take 1 / 2 tab daily for 4 days.\n",
            "0.00    2.14          2.14     - patient will report >3+ on the global rating of change in 4 - 6 sessions.    |||    the patient is agreeable to the plan of care and has no further questions.\n",
            "1.00    4.33          3.33     the patient is awake, alert, and oriented times three.    |||    patient is appreciative, understands and is in agreement with plan of care.\n",
            "4.00    1.91          2.09     thank you for contacting the xxx anticoagulation clinic.    |||    thank you for choosing the name, m.d.. care team for your health care needs!\n",
            "1.50    3.72          2.22     , family educational needs - family assessed : ready to learn, no apparent learning barriers.    |||    patient education : ready to learn, no apparent learning barriers were identified; learning preferences include listening.\n",
            "2.00    4.43          2.43     they verbalized agreement and understanding of the plan and contact information.    |||    discussed diagnosis and treatment plan; patient expressed understanding of the content.\n",
            "1.50    3.59          2.09     instructions : take 1 tab tid for 3 days, then take 1 tab bid for 3 days, then take 1 tab daily for 3 days, then take 1 / 2 tab daily for 4 days.    |||    instructions : take 2 tablets on first day followed 1 tablet daily on days 2 through 5. take medication for 5 days.\n",
            "0.00    2.21          2.21     the patient is agreeable to the plan of care and has no further questions.    |||    - patient will report >3+ on the global rating of change in 4 - 6 sessions.\n",
            "1.00    4.34          3.34     patient is appreciative, understands and is in agreement with plan of care.    |||    the patient is awake, alert, and oriented times three.\n",
            "4.00    1.90          2.10     thank you for choosing the name, m.d.. care team for your health care needs!    |||    thank you for contacting the xxx anticoagulation clinic.\n",
            "1.50    3.72          2.22     patient education : ready to learn, no apparent learning barriers were identified; learning preferences include listening.    |||    , family educational needs - family assessed : ready to learn, no apparent learning barriers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.8437083230684881\n",
            "Saved at:\n",
            "'/content/drive/My Drive/clinical-sts/models/new-bert-bio-bert-0.84.pth'\n",
            "'/content/drive/My Drive/clinical-sts/models/new-regression-bio-bert-0.84.pth'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QF1QpnxVoWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "0347fe1b-061c-414b-84a2-b0bb13729249"
      },
      "source": [
        "MODEL = \"/content/drive/My Drive/clinical-sts/umls-bert/\"\n",
        "my_bert = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert.cuda()\n",
        "\n",
        "regression_head = linearRegression().cuda()\n",
        "regression_optimizer = torch.optim.Adam(regression_head.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, output_hidden_states=True)\n",
        "train_dataloader, test_dataloader = load_data(tokenizer=tokenizer, batch_size=8)\n",
        "bert_config = BertConfig.from_pretrained(MODEL, output_hidden_states=True)\n",
        "my_bert_optimizer = AdamW(get_optimizer_params(my_bert), lr=1e-5)\n",
        "updated_model, regression_head = run_new_method(my_bert=my_bert, optimizer=my_bert_optimizer, # optimizer\n",
        "                                                regression_head=regression_head, \n",
        "                                                regression_optimizer=regression_optimizer,\n",
        "                                                train_dataloader=train_dataloader,\n",
        "                                                test_dataloader=test_dataloader,\n",
        "                                                epochs=5)\n",
        "\n",
        "pcc = evaluate_model(updated_model, regression_head, test_dataloader)\n",
        "model_name = \"umls-bert\"\n",
        "model_loc = \"/content/drive/My Drive/clinical-sts/models/newest-token-dnn-bert-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "regression_loc = \"/content/drive/My Drive/clinical-sts/models/newest-token-dnn-regression-{}-{:.4f}.pth\".format(model_name, pcc)\n",
        "torch.save(updated_model, model_loc)\n",
        "torch.save(regression_head, regression_loc)\n",
        "print(\"Saved at:\\n'{}'\\n'{}'\".format(model_loc, regression_loc))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/clinical-sts/umls-bert/ were not used when initializing BertModel: ['bert.embeddings.tui_type_embeddings.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Train Loss:1.0935211242927658\n",
            "Epoch: 0 Test Loss:0.5773112025289309\n",
            "\n",
            "yay, exit\n",
            "2.00    4.41          2.41     discussed diagnosis and treatment plan; patient expressed understanding of the content.    |||    they verbalized agreement and understanding of the plan and contact information.\n",
            "1.00    3.08          2.08     the patient is awake, alert, and oriented times three.    |||    patient is appreciative, understands and is in agreement with plan of care.\n",
            "4.50    2.46          2.04     she tolerated the procedure well and there no complications.    |||    the patient tolerated the procedure well and then was taken to the general care floor following the procedure for continued observation and monitoring.\n",
            "1.50    4.04          2.54     , family educational needs - family assessed : ready to learn, no apparent learning barriers.    |||    patient education : ready to learn, no apparent learning barriers were identified; learning preferences include listening.\n",
            "2.00    4.38          2.38     they verbalized agreement and understanding of the plan and contact information.    |||    discussed diagnosis and treatment plan; patient expressed understanding of the content.\n",
            "1.00    3.21          2.21     patient is appreciative, understands and is in agreement with plan of care.    |||    the patient is awake, alert, and oriented times three.\n",
            "4.50    2.41          2.09     the patient tolerated the procedure well and then was taken to the general care floor following the procedure for continued observation and monitoring.    |||    she tolerated the procedure well and there no complications.\n",
            "1.50    4.10          2.60     patient education : ready to learn, no apparent learning barriers were identified; learning preferences include listening.    |||    , family educational needs - family assessed : ready to learn, no apparent learning barriers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.8506290451136198\n",
            "Saved at:\n",
            "'/content/drive/My Drive/clinical-sts/models/new-token-dnn-bert-umls-bert-0.85.pth'\n",
            "'/content/drive/My Drive/clinical-sts/models/new-token-dnn-regression-umls-bert-0.85.pth'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O93E6J3kVGzx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}